{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c0d4f6-b7ea-4cde-9293-ed6274d7d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV dataset\n",
    "data_path = './dataset/countries_in_natural_language.csv' \n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Preview the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b8bf03-d7fa-4dbf-811e-c665188cae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Set the padding token if it's not defined\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Tokenize the 'Description' column\n",
    "df['tokens'] = df['Description'].apply(lambda x: tokenizer(x, truncation=True, padding='max_length', max_length=128))\n",
    "\n",
    "# Preview the tokenized data\n",
    "df['tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fd23ac-5844-4a3f-810a-2ca9307a70ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, df, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = df['Description'].tolist()\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"]\n",
    "        return tokenized_inputs\n",
    "\n",
    "# Initialize the dataset\n",
    "dataset = CustomTextDataset(tokenizer, df, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c598e8-a32d-44b0-a36c-2c325fbe43ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"accelerate>=0.26.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5988f5f6-238b-44b2-9848-fb9abea83b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "\n",
    "# Load pre-trained GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Set padding token if necessary\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',               # output directory\n",
    "    per_device_train_batch_size=2,        # batch size per device\n",
    "    num_train_epochs=3,                   # number of training epochs\n",
    "    logging_dir='./logs',                 # directory for storing logs\n",
    "    logging_steps=10                      # log every 10 steps\n",
    ")\n",
    "\n",
    "# Set up the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset  # Use your custom dataset here\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a407c6c0-067d-4a2f-bff6-a77812792584",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
